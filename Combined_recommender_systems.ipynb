{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preload Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import ast\n",
    "from scipy.stats import pearsonr\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsons(data_path, file):\n",
    "    \"\"\" helper function to load '.json' files (they're not proper jsons) \"\"\"\n",
    "    file_path = data_path + file\n",
    "    with open(file_path) as jsons:\n",
    "        lines = [json.loads(json_line) for json_line in jsons]\n",
    "    return pd.DataFrame(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = os.listdir('data/')\n",
    "\n",
    "review_file = 'review.json'\n",
    "business_file = 'business.json'\n",
    "user_file = 'user.json'\n",
    "tip_file = 'tip.json'\n",
    "checkin_file = 'checkin.json'   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, d = 0.75):\n",
    "    \"\"\"Split data in a training and test set.\n",
    "    \n",
    "    Arguments:\n",
    "    data -- any dataFrame.\n",
    "    d    -- the fraction of data in the training set\n",
    "    \"\"\"\n",
    "    np.random.seed(seed=5)\n",
    "    mask_test = np.random.rand(data.shape[0]) < d\n",
    "    return data[mask_test], data[~mask_test]  \n",
    "\n",
    "def pivot_categories(df):\n",
    "    \"\"\"Create a one-hot encoded matrix for genres.\n",
    "    \n",
    "    Arguments:\n",
    "    df -- a dataFrame containing at least the columns 'movieId' and 'genre'\n",
    "    \n",
    "    Output:\n",
    "    a matrix containing '0' or '1' in each cell.\n",
    "    1: the movie has the genre\n",
    "    0: the movie does not have the genre\n",
    "    \"\"\"\n",
    "    return df.pivot_table(index = 'business_id', columns = 'category', aggfunc = 'size', fill_value=0)\n",
    "\n",
    "def pivot_ratings(df):\n",
    "    \"\"\"Creates a utility matrix for user ratings for movies\n",
    "    \n",
    "    Arguments:\n",
    "    df -- a dataFrame containing at least the columns 'movieId' and 'genres'\n",
    "    \n",
    "    Output:\n",
    "    a matrix containing a rating in each cell. np.nan means that the user did not rate the movie\n",
    "    \"\"\"\n",
    "    return df.pivot(values='stars', columns='user_id', index='business_id')\n",
    "\n",
    "def create_similarity_matrix_jaccard(matrix):\n",
    "    m11 = matrix @ matrix.T\n",
    "    m10 = pd.DataFrame(matrix.sum(axis = 1).values + np.zeros(m11.shape), index = m11.index, columns = m11.index)\n",
    "    m01 = m10.T\n",
    "    return m11/(m01 + m10 - m11)\n",
    "\n",
    "def predict_ratings(similarity, utility, to_predict):\n",
    "    \"\"\"Predicts the predicted rating for the input test data.\n",
    "    \n",
    "    Arguments:\n",
    "    similarity -- a dataFrame that describes the similarity between items\n",
    "    utility    -- a dataFrame that contains a rating for each user (columns) and each movie (rows). \n",
    "                  If a user did not rate an item the value np.nan is assumed. \n",
    "    to_predict -- A dataFrame containing at least the columns movieId and userId for which to do the predictions\n",
    "    \"\"\"\n",
    "    # copy input (don't overwrite)\n",
    "    ratings_test_c = to_predict.copy()\n",
    "    # apply prediction to each row\n",
    "    ratings_test_c['predicted rating'] = to_predict.apply(lambda row: predict_ids(similarity, utility, row['user_id'], row['business_id']), axis=1)\n",
    "    return ratings_test_c\n",
    "\n",
    "### Helper functions for predict_ratings_item_based ###\n",
    "\n",
    "def predict_ids(similarity, utility, user_id, itemId):\n",
    "    # select right series from matrices and compute\n",
    "    if user_id in utility.columns and itemId in similarity.index:\n",
    "        return predict_vectors(utility.loc[:,user_id], similarity[itemId])\n",
    "    return 0\n",
    "\n",
    "def predict_vectors(user_ratings, similarities):\n",
    "    # select only businesses actually rated by user\n",
    "    relevant_ratings = user_ratings.dropna()\n",
    "    \n",
    "    # select corresponding similairties\n",
    "    similarities_s = similarities[relevant_ratings.index]\n",
    "    \n",
    "    # select neighborhood\n",
    "    similarities_s = similarities_s[similarities_s > 0.0]\n",
    "    relevant_ratings = relevant_ratings[similarities_s.index]\n",
    "    \n",
    "    # if there's nothing left return a prediction of 0\n",
    "    norm = similarities_s.sum()\n",
    "    if(norm == 0):\n",
    "        return 0\n",
    "    \n",
    "    # compute a weighted average (i.e. neighborhood is all) \n",
    "    return np.dot(relevant_ratings, similarities_s)/norm\n",
    "\n",
    "def unpack(bus_atr):\n",
    "    \n",
    "    attributes = pd.Series(bus_atr, dtype=object)\n",
    "    for bus_atr in attributes:\n",
    "        if isinstance(bus_atr, str):\n",
    "            attributes_index = attributes.index[0]\n",
    "            attributes = attributes.drop(attributes_index)\n",
    "            atr = ast.literal_eval(bus_atr)\n",
    "            \n",
    "            # Following part is a bit off, had an idea at first but\n",
    "            # changed it later, don't know how to efficiently rewrite it.\n",
    "            # So now, it's just add to dictionary and change to list.\n",
    "            if isinstance(atr, dict):\n",
    "                atr_series = pd.Series(atr, dtype=object)\n",
    "                attributes = attributes.append(atr_series)\n",
    "            else:\n",
    "                atr_series = pd.Series({attributes_index:atr}, dtype=object)\n",
    "                attributes = attributes.append(atr_series)\n",
    "    attributes = [k for k, v in attributes.to_dict().items() if v == True]\n",
    "    return attributes\n",
    "\n",
    "def extract_categories(businesses):\n",
    "    categories1 = businesses.apply(lambda row: pd.Series([row['business_id']] + row['categories'].lower().split(', ') + row['attributes']), axis=1)\n",
    "    categories_stack = categories1.set_index(0).stack()\n",
    "    categories_frame = categories_stack.to_frame()\n",
    "    categories_frame['business_id'] = categories_stack.index.droplevel(1)\n",
    "    categories_frame.columns = ['category', 'business_id']\n",
    "    categories_frame = categories_frame.reset_index()[['business_id', 'category']]\n",
    "    return categories_frame\n",
    "\n",
    "def pearson_distance(matrix, id1, id2):\n",
    "    # only take the features that have values for both id1 and id2\n",
    "    selected_features = matrix.loc[id1].notna() & matrix.loc[id2].notna()\n",
    "    \n",
    "    # if no matching features, return NaN\n",
    "    if not selected_features.any():\n",
    "        return np.nan\n",
    "    \n",
    "    # get the features from the matrix\n",
    "    features1 = matrix.loc[id1][selected_features]\n",
    "    features2 = matrix.loc[id2][selected_features]\n",
    "    \n",
    "    print('f1:',features1,'f2:',features2)\n",
    "    pearson_correlation_coefficient = pearsonr(features1,features2)[0]\n",
    "    \n",
    "    print(pearson_correlation_coefficient)\n",
    "    return pearson_correlation_coefficient\n",
    "\n",
    "def pearson_similarity(matrix, id1, id2):\n",
    "    \"\"\"Compute manhattan similarity between two rows.\"\"\"\n",
    "    # compute distance\n",
    "    similarity = pearson_distance(matrix, id1, id2)\n",
    "    \n",
    "    # if no distance could be computed (no shared features) return a similarity of 0\n",
    "    if similarity is np.nan:\n",
    "        return 0\n",
    "    \n",
    "    # else return similarity\n",
    "    return similarity\n",
    "\n",
    "def create_similarity_matrix_pearson(matrix):\n",
    "    \"\"\"creates the similarity matrix based on eucledian distance\"\"\"\n",
    "    similarity_matrix = pd.DataFrame(0, index=matrix.index, columns=matrix.index, dtype=float)\n",
    "    for index, row in similarity_matrix.iterrows():\n",
    "        for col in row.index:\n",
    "            similarity_matrix.at[index, col] = pearson_similarity(matrix, index, col)\n",
    "    return similarity_matrix\n",
    "\n",
    "def select_neighborhood(similarities, ratings, k):\n",
    "    neighborhood = similarities[similarities.index.isin(ratings[ratings.notnull()].index)].nlargest(n=k, keep='first')\n",
    "    neighborhood = neighborhood[neighborhood>0]\n",
    "    return neighborhood\n",
    "\n",
    "def weighted_mean(neighborhood, ratings):  \n",
    "    try:\n",
    "        return ((ratings[neighborhood.index] * neighborhood.values).values.sum()) / sum(neighborhood.values)\n",
    "    except:\n",
    "        return np.nan\n",
    "    \n",
    "def predict_ratings_item_based(similarity, utility, user_item_pairs):\n",
    "    ratings_test_c = user_item_pairs.copy()\n",
    "    ratings_test_c['predicted rating'] = pd.Series(0, index = review.index, dtype=float)\n",
    "    for value, row in ratings_test_c.iterrows():\n",
    "        business_id = row['business_id']\n",
    "        user_id = row['user_id']\n",
    "        neighborhood = select_neighborhood(similarity[business_id], utility[user_id], 100)\n",
    "        ratings_test_c.at[value, 'predicted rating'] = weighted_mean(neighborhood, utility[user_id])\n",
    "    return ratings_test_c \n",
    "\n",
    "# def recommended(predictions, treshold):\n",
    "#     for row in predictions.iterrows():\n",
    "#          predicted_item_based_recommended = predictions.loc[predictions['predicted rating'].sort_values(ascending = False)]\n",
    "#     return predicted_item_based_recommended[['user_id', 'business_id']][:treshold]\n",
    "\n",
    "def cosine_distance(matrix, id1, id2):\n",
    "    # only take the features that have values for both id1 and id2\n",
    "    selected_features = matrix.loc[id1].notna() & matrix.loc[id2].notna()\n",
    "    \n",
    "    # if no matching features, return NaN\n",
    "    if not selected_features.any():\n",
    "        return 0.0\n",
    "    \n",
    "    # get the features from the matrix\n",
    "    features1 = matrix.loc[id1][selected_features]\n",
    "    features2 = matrix.loc[id2][selected_features]\n",
    "\n",
    "    if (list(features1.unique()) == [0]) & (list(features2.unique()) == [0]):\n",
    "        return 1\n",
    "    \n",
    "    product = features1*features2\n",
    "    feature_squared_1 = features1**2\n",
    "    feature_squared_2 = features2**2\n",
    "\n",
    "    if ((np.sqrt(feature_squared_1.sum()))*(np.sqrt(feature_squared_2.sum()))) == 0:\n",
    "        return 0\n",
    "    \n",
    "    return product.sum()/((np.sqrt(feature_squared_1.sum()))*(np.sqrt(feature_squared_2.sum())))\n",
    "\n",
    "def create_similarity_matrix_cosine(matrix):\n",
    "    \"\"\" creates the similarity matrix based on cosine similarity \"\"\"\n",
    "    # TODO\n",
    "    similarity_matrix_cosine = pd.DataFrame(0, index=matrix.index, columns=matrix.index, dtype=float)\n",
    "    \n",
    "    # vul elke cel in met de cosine afstand\n",
    "    for x in similarity_matrix_cosine.index:\n",
    "        for y in similarity_matrix_cosine.columns:\n",
    "            similarity_matrix_cosine[y][x] = cosine_distance(matrix, y, x)\n",
    "    return similarity_matrix_cosine\n",
    "\n",
    "def mse(predicted_ratings):\n",
    "    \"\"\"Computes the mean square error between actual ratings and predicted ratings\n",
    "    \n",
    "    Arguments:\n",
    "    predicted_ratings -- a dataFrame containing the columns rating and predicted rating\n",
    "    \"\"\"\n",
    "    diff = predicted_ratings['stars'] - predicted_ratings['predicted rating']\n",
    "    return (diff**2).mean()\n",
    "\n",
    "def rmse(predicted_ratings):\n",
    "    \"\"\"Computes the mean square error between actual ratings and predicted ratings\n",
    "    \n",
    "    Arguments:\n",
    "    predicted_ratings -- a dataFrame containing the columns rating and predicted rating\n",
    "    \"\"\"\n",
    "    diff = predicted_ratings['stars'] - predicted_ratings['predicted rating']\n",
    "    return ((diff**2)**0.5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset load and Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_cities = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.concat([load_jsons('./data/' + city + '/', review_file) for city in cities[:number_of_cities]])\n",
    "reviews = reviews.sort_values('stars').drop_duplicates(subset=['user_id', 'business_id'], keep='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses = pd.concat([load_jsons('./data/' + city + '/', business_file) for city in cities[:number_of_cities]])\n",
    "businesses = businesses[businesses['categories'].str.contains('Restaurants', na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "restaurant_ids = businesses.business_id.tolist()\n",
    "reviews = reviews[reviews['business_id'].isin(restaurant_ids)]\n",
    "reviews = reviews[reviews.groupby('user_id')['user_id'].transform('size') > 3]\n",
    "reviews_training, reviews_test = split_data(reviews, d=0.80)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility matrixes for both algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utility_reviews = pivot_ratings(reviews_training)\n",
    "display(utility_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_utility = pivot_ratings(reviews)\n",
    "display(total_utility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CbF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "businesses2 = businesses.copy()\n",
    "businesses2.index = range(businesses.shape[0])\n",
    "for i in range(businesses.shape[0]):\n",
    "    unpacked = unpack(businesses['attributes'].iloc[i])\n",
    "    businesses2.at[i, 'attributes'] = unpacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "businesses_cats = extract_categories(businesses2)\n",
    "businesses_cats_matrix = pivot_categories(businesses_cats)\n",
    "businesses_cats_matrix = businesses_cats_matrix.drop(columns=['restaurants'])\n",
    "businesses_cats_matrix.drop([col for col, val in businesses_cats_matrix.sum().iteritems() if val < businesses2.shape[0]/100], axis=1, inplace=True)\n",
    "\n",
    "display(businesses_cats_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_categories = create_similarity_matrix_jaccard(businesses_cats_matrix)\n",
    "display(similarity_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predicted_ratings_cbf = predict_ratings(similarity_categories, utility_reviews, reviews_test[['user_id', 'business_id', 'stars']])\n",
    "display(predicted_ratings_cbf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Item-based CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "utility_imputed = mp.fit(utility_reviews).transform(utility_reviews)\n",
    "\n",
    "pearson_sim = 1-pairwise_distances(utility_imputed, metric=\"correlation\")\n",
    "\n",
    "similarity_cf = pd.DataFrame(pearson_sim, index=utility_reviews.index, columns=utility_reviews.index)\n",
    "display(similarity_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "similarity_scaled = scaler.fit(similarity_cf).transform(similarity_cf)\n",
    "display(pd.DataFrame(similarity_scaled,index=utility_reviews.index, columns=utility_reviews.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings_cf = predict_ratings(similarity_cf, utility_reviews, reviews_test[['user_id', 'business_id', 'stars']])\n",
    "display(predicted_ratings_cf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ratings_rndm_test = reviews_test.copy()\n",
    "ratings_rndm_test['predicted rating'] = np.random.uniform(0,5,size=len(ratings_rndm_test))\n",
    "mse_random = mse(ratings_rndm_test)\n",
    "\n",
    "mse_mean_cf = mse(predicted_ratings_cf)\n",
    "mse_mean_cbf = mse(predicted_ratings_cbf)\n",
    "\n",
    "print(f'MSE for random prediction: {mse_random:.2f}')\n",
    "print(f'MSE for content based filtering prediction : {mse_mean_cbf:.2f}')\n",
    "print(f'MSE for item-based collaborative filtering prediction : {mse_mean_cf:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_random = rmse(ratings_rndm_test)\n",
    "\n",
    "rmse_mean_cf = rmse(predicted_ratings_cf)\n",
    "rmse_mean_cbf = rmse(predicted_ratings_cbf)\n",
    "\n",
    "print(f'RMSE for random prediction: {rmse_random:.2f}')\n",
    "print(f'RMSE for content based filtering prediction : {rmse_mean_cbf:.2f}')\n",
    "print(f'RMSE for item-based collaborative filtering prediction : {rmse_mean_cf:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intra-list Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_user2 = reviews.user_id.unique()\n",
    "random_user = np.random.choice(random_user2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_ids = businesses.business_id\n",
    "column_names = [\"user_id\", \"business_id\"]\n",
    "test = pd.DataFrame(columns = column_names)\n",
    "test.business_id = business_ids\n",
    "test['user_id'] = random_user\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "test_all = test.merge(reviews[['user_id', 'business_id']], on=['user_id', 'business_id'], how='left', indicator=True)\n",
    "seen = test_all[test_all['_merge'] == 'both']['business_id']\n",
    "\n",
    "for value in seen.values:\n",
    "    test = test[test['business_id'] != value]\n",
    "\n",
    "test_cbf = test.copy()   \n",
    "test_cf = test.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cbf['predicted rating'] = test.apply(lambda row: predict_ids(similarity_categories, total_utility, row['user_id'], row['business_id']), axis=1)\n",
    "results_cbf = test_cbf.sort_values(by= 'predicted rating', ascending=False).head(20)\n",
    "\n",
    "print('Best 20 results for CBF')\n",
    "display(results_cbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cf['predicted rating'] = test.apply(lambda row: predict_ids(similarity_cf, total_utility, row['user_id'], row['business_id']), axis=1)\n",
    "results_cf = test_cf.sort_values(by= 'predicted rating', ascending=False).head(20)\n",
    "\n",
    "print('Best 20 results for CF')\n",
    "display(results_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tests = businesses_cats_matrix.loc[results_cbf['business_id'].values]\n",
    "similair = create_similarity_matrix_jaccard(tests)\n",
    "lijst_similair_values_cbf = similair.mean().sort_values(ascending=False)\n",
    "display(lijst_similair_values_cbf)\n",
    "print('Mean similarity for CbF: ', lijst_similair_values_cbf.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = businesses_cats_matrix.loc[results_cf['business_id'].values]\n",
    "similair = create_similarity_matrix_jaccard(tests)\n",
    "lijst_similair_values_cf = similair.mean().sort_values(ascending=False)\n",
    "display(lijst_similair_values_cf)\n",
    "print('Mean similarity for CF: ', lijst_similair_values_cf.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
